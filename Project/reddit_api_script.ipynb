{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title : Sentiment analysis on Syrian Civil War Conflict using reddit\n",
    "## API Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Obtaining dependency information for praw from https://files.pythonhosted.org/packages/81/6a/21bc058bcccbe03f6a0895bf1bd60c805f0c526aa4e9bfaac775ed0b299c/praw-7.7.1-py3-none-any.whl.metadata\n",
      "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting prawcore<3,>=2.1 (from praw)\n",
      "  Obtaining dependency information for prawcore<3,>=2.1 from https://files.pythonhosted.org/packages/96/5c/8af904314e42d5401afcfaff69940dc448e974f80f7aa39b241a4fbf0cf1/prawcore-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update-checker>=0.18 (from praw)\n",
      "  Obtaining dependency information for update-checker>=0.18 from https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\krish\\anaconda\\lib\\site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\krish\\anaconda\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
      "Requirement already satisfied: six in c:\\users\\krish\\anaconda\\lib\\site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krish\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krish\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krish\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krish\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.2.2)\n",
      "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
      "   ---------------------------------------- 0.0/191.0 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 61.4/191.0 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 174.1/191.0 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 191.0/191.0 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update-checker, prawcore, praw\n",
      "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\krish\\anaconda\\lib\\site-packages (7.7.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in c:\\users\\krish\\anaconda\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in c:\\users\\krish\\anaconda\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\krish\\anaconda\\lib\\site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\krish\\anaconda\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
      "Requirement already satisfied: six in c:\\users\\krish\\anaconda\\lib\\site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krish\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krish\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krish\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krish\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw\n",
    "import praw\n",
    "reddit = praw.Reddit(\n",
    "    client_id = \"074QJF4ao9yCbvOveQvkUg\",\n",
    "    client_secret=\"R5UGwDeZdUwhMq8fXpDIIfNR3Imx0w\",\n",
    "    user_agent = \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"AskMiddleEast\")\n",
    "limit_per_request = 1000\n",
    "total_limit = 10000\n",
    "all_posts = []\n",
    "\n",
    "# Fetch posts in chunks until the desired total limit is reached\n",
    "while len(all_posts) < total_limit:\n",
    "    # Calculate the remaining limit for the current request\n",
    "    remaining_limit = total_limit - len(all_posts)\n",
    "    \n",
    "    # Adjust the limit for the current request based on the remaining limit\n",
    "    current_limit = min(limit_per_request, remaining_limit)\n",
    "    \n",
    "    # Fetch the top posts using the current limit\n",
    "    top_posts = subreddit.new(limit=current_limit)\n",
    "    \n",
    "    # Extend the list of all_posts with the current set of posts\n",
    "    all_posts.extend(top_posts)\n",
    "    \n",
    "    # Print a message to indicate progress (optional)\n",
    "    print(f\"Fetched {len(all_posts)} out of {total_limit} posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data = []\n",
    "count = 0\n",
    "for post in all_posts:\n",
    "    post_info = {\n",
    "        \n",
    "        \"title\": post.title,\n",
    "        \"score\": post.score,\n",
    "        \"id\": post.id,\n",
    "        \"url\": post.url,\n",
    "        \"comms_num\": post.num_comments,\n",
    "        \"created\": post.created_utc,\n",
    "        \"body\": post.selftext,\n",
    "        \"timestamp\": post.created_utc,\n",
    "        \"subreddit\" : post.subreddit.display_name\n",
    "    }\n",
    "    posts_data.append(post_info)\n",
    "    count += 1\n",
    "df = pd.DataFrame(posts_data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the csv names to be changed based on sub reddit we use, i used the following subreddit for data sourcing\n",
    "#“AskMiddleEast”, “SyrianCirclejerkWar”, “SyrianRebels”, \"syriancivilwar\"”, “syrianconflict”.\n",
    "df.to_csv('SyrianCivilWar.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
